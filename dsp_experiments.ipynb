{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# DSP experiments\n",
        "\n",
        "This notebook contains DSP experiments to assess the accuracy of confidence level classification of climate statements by various LLMs."
      ],
      "metadata": {
        "id": "5Yr_qsY-IVDh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "McPATs55HhCb",
        "outputId": "90cbd37c-9eb8-4ce0-eef9-bc2a60c9fff7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'cs224u'...\n",
            "remote: Enumerating objects: 2216, done.\u001b[K\n",
            "remote: Counting objects: 100% (124/124), done.\u001b[K\n",
            "remote: Compressing objects: 100% (78/78), done.\u001b[K\n",
            "remote: Total 2216 (delta 55), reused 83 (delta 46), pack-reused 2092\u001b[K\n",
            "Receiving objects: 100% (2216/2216), 41.49 MiB | 19.22 MiB/s, done.\n",
            "Resolving deltas: 100% (1354/1354), done.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/stanfordnlp/dsp (from -r cs224u/requirements.txt (line 15))\n",
            "  Cloning https://github.com/stanfordnlp/dsp to /tmp/pip-req-build-x32h96un\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/stanfordnlp/dsp /tmp/pip-req-build-x32h96un\n",
            "  Resolved https://github.com/stanfordnlp/dsp to commit d4e6c1ac7f54986989640d5fb46108f0bd9cc802\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from -r cs224u/requirements.txt (line 1)) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from -r cs224u/requirements.txt (line 2)) (1.10.1)\n",
            "Requirement already satisfied: matplotlib>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from -r cs224u/requirements.txt (line 3)) (3.7.1)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from -r cs224u/requirements.txt (line 4)) (1.2.2)\n",
            "Requirement already satisfied: nltk>=3.7 in /usr/local/lib/python3.10/dist-packages (from -r cs224u/requirements.txt (line 5)) (3.8.1)\n",
            "Requirement already satisfied: pytest>=7.1 in /usr/local/lib/python3.10/dist-packages (from -r cs224u/requirements.txt (line 6)) (7.2.2)\n",
            "Collecting jupyter>=1.0.0 (from -r cs224u/requirements.txt (line 7))\n",
            "  Downloading jupyter-1.0.0-py2.py3-none-any.whl (2.7 kB)\n",
            "Requirement already satisfied: pandas>=1.5 in /usr/local/lib/python3.10/dist-packages (from -r cs224u/requirements.txt (line 8)) (1.5.3)\n",
            "Collecting torch==1.13.1 (from -r cs224u/requirements.txt (line 9))\n",
            "  Downloading torch-1.13.1-cp310-cp310-manylinux1_x86_64.whl (887.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.5/887.5 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.14.1 (from -r cs224u/requirements.txt (line 10))\n",
            "  Downloading torchvision-0.14.1-cp310-cp310-manylinux1_x86_64.whl (24.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.2/24.2 MB\u001b[0m \u001b[31m61.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers==4.26.1 (from -r cs224u/requirements.txt (line 11))\n",
            "  Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m96.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets==2.10.1 (from -r cs224u/requirements.txt (line 12))\n",
            "  Downloading datasets-2.10.1-py3-none-any.whl (469 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 kB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting spacy==3.5.1 (from -r cs224u/requirements.txt (line 13))\n",
            "  Downloading spacy-3.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m73.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cohere (from -r cs224u/requirements.txt (line 16))\n",
            "  Downloading cohere-4.6.0-py3-none-any.whl (33 kB)\n",
            "Collecting openai (from -r cs224u/requirements.txt (line 17))\n",
            "  Downloading openai-0.27.7-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.13.1->-r cs224u/requirements.txt (line 9)) (4.5.0)\n",
            "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==1.13.1->-r cs224u/requirements.txt (line 9))\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96 (from torch==1.13.1->-r cs224u/requirements.txt (line 9))\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66 (from torch==1.13.1->-r cs224u/requirements.txt (line 9))\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==1.13.1->-r cs224u/requirements.txt (line 9))\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m68.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision==0.14.1->-r cs224u/requirements.txt (line 10)) (2.27.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.14.1->-r cs224u/requirements.txt (line 10)) (8.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.26.1->-r cs224u/requirements.txt (line 11)) (3.12.0)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0 (from transformers==4.26.1->-r cs224u/requirements.txt (line 11))\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.26.1->-r cs224u/requirements.txt (line 11)) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.26.1->-r cs224u/requirements.txt (line 11)) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.26.1->-r cs224u/requirements.txt (line 11)) (2022.10.31)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.26.1->-r cs224u/requirements.txt (line 11))\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m102.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.26.1->-r cs224u/requirements.txt (line 11)) (4.65.0)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.10.1->-r cs224u/requirements.txt (line 12)) (9.0.0)\n",
            "Collecting dill<0.3.7,>=0.3.0 (from datasets==2.10.1->-r cs224u/requirements.txt (line 12))\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting xxhash (from datasets==2.10.1->-r cs224u/requirements.txt (line 12))\n",
            "  Downloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets==2.10.1->-r cs224u/requirements.txt (line 12))\n",
            "  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets==2.10.1->-r cs224u/requirements.txt (line 12)) (2023.4.0)\n",
            "Collecting aiohttp (from datasets==2.10.1->-r cs224u/requirements.txt (line 12))\n",
            "  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m52.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting responses<0.19 (from datasets==2.10.1->-r cs224u/requirements.txt (line 12))\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy==3.5.1->-r cs224u/requirements.txt (line 13)) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.5.1->-r cs224u/requirements.txt (line 13)) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.5.1->-r cs224u/requirements.txt (line 13)) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy==3.5.1->-r cs224u/requirements.txt (line 13)) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy==3.5.1->-r cs224u/requirements.txt (line 13)) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy==3.5.1->-r cs224u/requirements.txt (line 13)) (8.1.9)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy==3.5.1->-r cs224u/requirements.txt (line 13)) (1.1.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy==3.5.1->-r cs224u/requirements.txt (line 13)) (2.4.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy==3.5.1->-r cs224u/requirements.txt (line 13)) (2.0.8)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.5.1->-r cs224u/requirements.txt (line 13)) (0.7.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.5.1->-r cs224u/requirements.txt (line 13)) (0.10.1)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy==3.5.1->-r cs224u/requirements.txt (line 13)) (6.3.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy==3.5.1->-r cs224u/requirements.txt (line 13)) (1.10.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy==3.5.1->-r cs224u/requirements.txt (line 13)) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy==3.5.1->-r cs224u/requirements.txt (line 13)) (67.7.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.5.1->-r cs224u/requirements.txt (line 13)) (3.3.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1->-r cs224u/requirements.txt (line 9)) (0.40.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->-r cs224u/requirements.txt (line 3)) (1.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->-r cs224u/requirements.txt (line 3)) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->-r cs224u/requirements.txt (line 3)) (4.39.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->-r cs224u/requirements.txt (line 3)) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->-r cs224u/requirements.txt (line 3)) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->-r cs224u/requirements.txt (line 3)) (2.8.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.2->-r cs224u/requirements.txt (line 4)) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.2->-r cs224u/requirements.txt (line 4)) (3.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.7->-r cs224u/requirements.txt (line 5)) (8.1.3)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from pytest>=7.1->-r cs224u/requirements.txt (line 6)) (23.1.0)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest>=7.1->-r cs224u/requirements.txt (line 6)) (2.0.0)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest>=7.1->-r cs224u/requirements.txt (line 6)) (1.0.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest>=7.1->-r cs224u/requirements.txt (line 6)) (1.1.1)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytest>=7.1->-r cs224u/requirements.txt (line 6)) (2.0.1)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.10/dist-packages (from jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (6.4.8)\n",
            "Collecting qtconsole (from jupyter>=1.0.0->-r cs224u/requirements.txt (line 7))\n",
            "  Downloading qtconsole-5.4.3-py3-none-any.whl (121 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.9/121.9 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jupyter-console in /usr/local/lib/python3.10/dist-packages (from jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (6.1.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.10/dist-packages (from jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (6.5.4)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.10/dist-packages (from jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (5.5.6)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.10/dist-packages (from jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (7.7.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.5->-r cs224u/requirements.txt (line 8)) (2022.7.1)\n",
            "Collecting backoff (from dsp-ml==0.1.5->-r cs224u/requirements.txt (line 15))\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Collecting ujson (from dsp-ml==0.1.5->-r cs224u/requirements.txt (line 15))\n",
            "  Downloading ujson-5.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (52 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.8/52.8 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.10.1->-r cs224u/requirements.txt (line 12)) (2.0.12)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets==2.10.1->-r cs224u/requirements.txt (line 12))\n",
            "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->datasets==2.10.1->-r cs224u/requirements.txt (line 12))\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets==2.10.1->-r cs224u/requirements.txt (line 12))\n",
            "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp->datasets==2.10.1->-r cs224u/requirements.txt (line 12))\n",
            "  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp->datasets==2.10.1->-r cs224u/requirements.txt (line 12))\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.7.0->-r cs224u/requirements.txt (line 3)) (1.16.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.14.1->-r cs224u/requirements.txt (line 10)) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.14.1->-r cs224u/requirements.txt (line 10)) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.14.1->-r cs224u/requirements.txt (line 10)) (3.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy==3.5.1->-r cs224u/requirements.txt (line 13)) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy==3.5.1->-r cs224u/requirements.txt (line 13)) (0.0.4)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (0.2.0)\n",
            "Requirement already satisfied: ipython>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (7.34.0)\n",
            "Requirement already satisfied: traitlets>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (5.7.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (6.1.12)\n",
            "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (6.3.1)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (3.6.4)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (3.0.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy==3.5.1->-r cs224u/requirements.txt (line 13)) (2.1.2)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-console->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (3.0.38)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from jupyter-console->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (2.14.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (4.9.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (4.11.2)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (6.0.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (0.7.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (0.4)\n",
            "Requirement already satisfied: jupyter-core>=4.7 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (5.3.0)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (0.2.2)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (0.8.4)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (0.7.4)\n",
            "Requirement already satisfied: nbformat>=5.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (5.8.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (1.5.0)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (1.2.1)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (23.2.1)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (21.3.0)\n",
            "Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (1.5.6)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (1.8.0)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (0.17.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (0.16.0)\n",
            "Collecting qtpy>=2.0.1 (from qtconsole->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7))\n",
            "  Downloading QtPy-2.3.1-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.9/84.9 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jedi>=0.16 (from ipython>=5.0.0->ipykernel->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7))\n",
            "  Downloading jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m66.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (0.7.5)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (4.8.0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.7->nbconvert->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (3.3.0)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.1->nbconvert->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (2.16.3)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.1->nbconvert->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (4.3.3)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->jupyter-console->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (0.2.6)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.10/dist-packages (from terminado>=0.8.3->notebook->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (0.7.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi->notebook->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (21.2.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (2.4.1)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->nbconvert->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (0.5.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=5.0.0->ipykernel->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (0.8.3)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (0.19.3)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (2.21)\n",
            "Building wheels for collected packages: dsp-ml\n",
            "  Building wheel for dsp-ml (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dsp-ml: filename=dsp_ml-0.1.5-py3-none-any.whl size=37803 sha256=67e8ef8a951998d881b513331e8e646ba9bb856d502b00c7e023c14c330f73d7\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-bi_4gi9t/wheels/b9/cc/21/df1431394d64701ffb9d4a5a20afe701a19275c3090b73fd0e\n",
            "Successfully built dsp-ml\n",
            "Installing collected packages: tokenizers, xxhash, ujson, qtpy, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cublas-cu11, multidict, jedi, frozenlist, dill, backoff, async-timeout, yarl, responses, nvidia-cudnn-cu11, multiprocess, huggingface-hub, aiosignal, transformers, torch, aiohttp, torchvision, spacy, qtconsole, openai, cohere, datasets, jupyter, dsp-ml\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.0.1+cu118\n",
            "    Uninstalling torch-2.0.1+cu118:\n",
            "      Successfully uninstalled torch-2.0.1+cu118\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.15.2+cu118\n",
            "    Uninstalling torchvision-0.15.2+cu118:\n",
            "      Successfully uninstalled torchvision-0.15.2+cu118\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.5.2\n",
            "    Uninstalling spacy-3.5.2:\n",
            "      Successfully uninstalled spacy-3.5.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.0.2+cu118 requires torch==2.0.1, but you have torch 1.13.1 which is incompatible.\n",
            "torchdata 0.6.1 requires torch==2.0.1, but you have torch 1.13.1 which is incompatible.\n",
            "torchtext 0.15.2 requires torch==2.0.1, but you have torch 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 backoff-2.2.1 cohere-4.6.0 datasets-2.10.1 dill-0.3.6 dsp-ml-0.1.5 frozenlist-1.3.3 huggingface-hub-0.14.1 jedi-0.18.2 jupyter-1.0.0 multidict-6.0.4 multiprocess-0.70.14 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 openai-0.27.7 qtconsole-5.4.3 qtpy-2.3.1 responses-0.18.0 spacy-3.5.1 tokenizers-0.13.3 torch-1.13.1 torchvision-0.14.1 transformers-4.26.1 ujson-5.7.0 xxhash-3.2.0 yarl-1.9.2\n"
          ]
        }
      ],
      "source": [
        "try: \n",
        "    # This library is our indicator that the required installs\n",
        "    # need to be done.\n",
        "    import datasets\n",
        "    root_path = '.'\n",
        "except ModuleNotFoundError:\n",
        "    !git clone https://github.com/cgpotts/cs224u/\n",
        "    !pip install -r cs224u/requirements.txt\n",
        "    root_path = 'dsp'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we import modules to work with DSP, OpenAI, HuggingFace, and Cohere models"
      ],
      "metadata": {
        "id": "mOyqFOZQJ3X1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cohere\n",
        "from datasets import load_dataset\n",
        "import openai\n",
        "import re\n",
        "import os\n",
        "import dsp\n",
        "import transformers"
      ],
      "metadata": {
        "id": "cOdafIXAJBbA"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we import key variables such as API keys. Steps:\n",
        "\n",
        "1) Create a keys.txt file in the root of your /content/ folder in the Colab instance\n",
        "\n",
        "2) Define the API keys you will use \n",
        "\n",
        "3) Run the cell below to importan"
      ],
      "metadata": {
        "id": "i4FR-mNlJ9rw"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mAz6IUbVkZlt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the contents of the file\n",
        "with open('/content/keys.txt', 'r') as file:\n",
        "    file_contents = file.read()\n",
        "\n",
        "# Assign the variables\n",
        "exec(file_contents)\n",
        "\n",
        "os.environ[\"DSP_NOTEBOOK_CACHEDIR\"] = os.path.join(root_path, 'cache')\n",
        "openai_key = OPENAI_API_KEY  \n",
        "cohere_key = COHERE_API_KEY  "
      ],
      "metadata": {
        "id": "92qnhnLnJGmj"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model setting\n",
        "\n",
        "Now we define a choice of LM and setup the DSP environment."
      ],
      "metadata": {
        "id": "r4BukR4-LE_h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## We use the chatGPT AR-LM model\n",
        "\n",
        "lm = dsp.GPT3(model = \"gpt-3.5-turbo\", api_key=openai_key, model_type=\"chat\")\n",
        "dsp.settings.configure(lm=lm)\n",
        "dsp.settings.show_guidelines = True"
      ],
      "metadata": {
        "id": "R-qXw7kxLPYp"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the dataset\n",
        "Upload a tab separated version of the dataset to /content/."
      ],
      "metadata": {
        "id": "4viPCioLLwRf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd \n",
        "\n",
        "df = pd.read_csv('dataset.tsv', sep='\\t', skiprows=0)\n",
        "df_use = df[df['Use?'] == 1]\n",
        "dataset = df_use[['text', 'confidence_rating']]\n",
        "dataset.tail"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OssAa2klLvzW",
        "outputId": "f5a7a5a6-bcf3-4aef-929a-6622ed963dfe"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method NDFrame.tail of                                                    text confidence_rating\n",
              "1     Mid-latitude storm tracks have likely shifted ...            medium\n",
              "12    Human-induced climate change has contributed t...            medium\n",
              "14    Increases in West African monsoon precipitatio...            medium\n",
              "15    Event attribution studies and physical underst...              high\n",
              "18    The observed average rate of heating of the cl...              high\n",
              "...                                                 ...               ...\n",
              "7439  Transport-related emissions in developing regi...              high\n",
              "7442  Increased efficiency has been insufficient to ...              high\n",
              "7493  The extent to which countries increase the amb...              high\n",
              "7507  A significant push for international climate f...              high\n",
              "7859  In short, future urban expansion will amplify ...         very high\n",
              "\n",
              "[298 rows x 2 columns]>"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ipcc_sent = [dsp.Example(input=row['text'], label=row['confidence_rating'])\n",
        "                    for _, row in dataset.iterrows()]"
      ],
      "metadata": {
        "id": "-21GN7mGX17J"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ipcc_sent[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bMrlFsaPOJU-",
        "outputId": "277c9a94-48e2-4f16-e139-51d23a270979"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'input': 'Mid-latitude storm tracks have likely shifted poleward in both hemispheres since the 1980s, with marked seasonality in trends ',\n",
              "  'label': 'medium'},\n",
              " {'input': 'Human-induced climate change has contributed to increases in agricultural and ecological droughts15 in some regions due to increased land evapotranspiration16',\n",
              "  'label': 'medium'},\n",
              " {'input': 'Increases in West African monsoon precipitation since the 1980s are partly due to the growing influence of GHGs and reductions in the cooling effect of human-caused aerosol emissions over Europe and North America',\n",
              "  'label': 'medium'},\n",
              " {'input': 'Event attribution studies and physical understanding indicate that human-induced climate change increases heavy precipitation associated with tropical cyclones',\n",
              "  'label': 'high'},\n",
              " {'input': 'The observed average rate of heating of the climate system increased from 0.50 [0.32 to 0.69] W m–2 for the period 1971–200619 to 0.79 [0.52 to 1.06] W m–2 for the period 2006–201820',\n",
              "  'label': 'high'},\n",
              " {'input': 'Together, ice-sheet and glacier mass loss were the dominant contributors to global mean sea level rise during 2006–2018 ',\n",
              "  'label': 'high'},\n",
              " {'input': 'Increases in frequency and intensity of hydrological droughts become larger with increasing global warming in some regions',\n",
              "  'label': 'medium'},\n",
              " {'input': 'The Arctic is projected to experience the highest increase in the temperature of the coldest days, at about three times the rate of global warming ',\n",
              "  'label': 'high'},\n",
              " {'input': 'At the global scale, extreme daily precipitation events are projected to intensify by about 7% for each 1°C of global warming ',\n",
              "  'label': 'high'},\n",
              " {'input': 'Over the next 2000 years, global mean sea level will rise by about 2 to 3 m if warming is limited to 1.5°C, 2 to 6 m if limited to 2°C and 19 to 22 m with 5°C of warming, and it will continue to rise over subsequent millennia ',\n",
              "  'label': 'low'}]"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(ipcc_sent))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3tmicIyUUIj",
        "outputId": "9c5a563a-93da-46d2-abf5-da17c0e91745"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "298\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining templates\n",
        "\n",
        "### Zero-shot learning\n",
        "Testing how models do on the task right out of the gate."
      ],
      "metadata": {
        "id": "JcN630LpOWaI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Input = dsp.Type(\n",
        "    prefix=\"Statement:\", \n",
        "    desc=\"${the climate factoid the confidence level of which is to be assessed.}\")\n",
        "\n",
        "Label = dsp.Type(\n",
        "    prefix=\"Confidence:\", \n",
        "    desc=\"${a confidence level: 'low', 'medium', 'high', or 'very high'}\")\n",
        "\n",
        "zero_shot_template = dsp.Template(\n",
        "    instructions=\"You are a helpful climate science and policy assistant, which accurately assesses the certainty level of facts about climate change extracted from the IPCC Assessment Reports.\\n\\nFor each statement presented to you, you will label the statement with exactly one of the following degrees of confidence: low, medium, high, or very high confidence.\",\n",
        "    input=Input(),\n",
        "    output=Label())"
      ],
      "metadata": {
        "id": "8oiVbQ94OYYd"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example:"
      ],
      "metadata": {
        "id": "wORGK6fTZsjC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ex = dsp.Example(\n",
        "    input=ipcc_sent[0]['input'], label=ipcc_sent[0]['label'])\n",
        "\n",
        "ex.demos=dsp.sample(ipcc_sent, 0)\n",
        "\n",
        "print(zero_shot_template(ex))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WqQ75vTGZt03",
        "outputId": "a45c57ee-fa3c-4bb9-da5b-90ad6daad015"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are a helpful climate science and policy assistant, which accurately assesses the certainty level of facts about climate change extracted from the IPCC Assessment Reports.\n",
            "\n",
            "For each statement presented to you, you will label the statement with exactly one of the following degrees of confidence: low, medium, high, or very high confidence.\n",
            "\n",
            "---\n",
            "\n",
            "Follow the following format.\n",
            "\n",
            "Statement: ${the climate factoid the confidence level of which is to be assessed.}\n",
            "Confidence: ${a confidence level: 'low', 'medium', 'high', or 'very high'}\n",
            "\n",
            "---\n",
            "\n",
            "Statement: Mid-latitude storm tracks have likely shifted poleward in both hemispheres since the 1980s, with marked seasonality in trends\n",
            "Confidence:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining the task\n",
        "\n",
        "With zero shot we simply provide instructions and provide a format template, with no training demonstrations."
      ],
      "metadata": {
        "id": "ONZfaTzxZja0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dsp.transformation\n",
        "def assess_certainty(example): \n",
        "    \n",
        "    example, completion = dsp.generate(zero_shot_template)(example, stage='zero-shot')\n",
        "\n",
        "    return completion"
      ],
      "metadata": {
        "id": "I7VWLcOdPmXK"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing our task on the first sentence of the dataset: gpt3.5-turbo gets this first task right!"
      ],
      "metadata": {
        "id": "VW71OwRIeqgT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "assess_certainty(ex).label"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "uM3jPoEEeX9_",
        "outputId": "6aae0e7a-1655-4f63-e1ae-474284597aec"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'medium'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# First experiment\n",
        "\n",
        "We assess the certainty level of the LM on the dataset, filter the output for extra \"confidence\" etc., then compare with ground truth."
      ],
      "metadata": {
        "id": "JPfSCzP2fAkL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "samples = dataset #.sample(50)"
      ],
      "metadata": {
        "id": "rB8fieaReqPg"
      },
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "samples['prediction'] = samples.text.apply(\n",
        "    lambda x: re.findall(r\"\\b(?:low|medium|high|very high)\\b\", assess_certainty(dsp.Example(input=x, demos=dsp.sample(ipcc_sent, 0))).output.lower())[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OD0cALUafsUo",
        "outputId": "3bda23df-c4fe-4ef8-ace1-77c96c64c43f"
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:backoff:Backing off request(...) for 0.5s (openai.error.RateLimitError: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID cc9de231e0c01c8e644373171ef01549 in your message.))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Backing off 0.5 seconds after 1 tries calling function <function GPT3.request at 0x7f7a72808ee0> with kwargs {}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:backoff:Backing off request(...) for 0.2s (openai.error.RateLimitError: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID a83415f7621072f4f48d850d117e0c2b in your message.))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Backing off 0.2 seconds after 1 tries calling function <function GPT3.request at 0x7f7a72808ee0> with kwargs {}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:backoff:Backing off request(...) for 1.0s (openai.error.RateLimitError: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID f05f29343676acf3ad72e74f39265bea in your message.))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Backing off 1.0 seconds after 1 tries calling function <function GPT3.request at 0x7f7a72808ee0> with kwargs {}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:backoff:Backing off request(...) for 1.0s (openai.error.RateLimitError: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 7981c552196a8f7e26a8849a0889f4e8 in your message.))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Backing off 1.0 seconds after 1 tries calling function <function GPT3.request at 0x7f7a72808ee0> with kwargs {}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:backoff:Backing off request(...) for 0.6s (openai.error.RateLimitError: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 485ee1dd93c41912a2ae7834d29ecff8 in your message.))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Backing off 0.6 seconds after 1 tries calling function <function GPT3.request at 0x7f7a72808ee0> with kwargs {}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-145-e03057c783cb>:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  samples['prediction'] = samples.text.apply(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "samples['correct'] = samples.apply(\n",
        "    lambda row: (row['confidence_rating'] == row['prediction']), axis=1)\n",
        "\n",
        "samples['correct'].sum() / samples.shape[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "shRBi3TugMX9",
        "outputId": "bcc17112-9271-4528-ada1-456dc9cb42de"
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-146-0eef760c6b07>:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  samples['correct'] = samples.apply(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3624161073825503"
            ]
          },
          "metadata": {},
          "execution_count": 146
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "samples.head"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9TR380Bgk_S",
        "outputId": "d05923f4-fa42-433e-993e-37959cd5d16f"
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method NDFrame.head of                                                    text confidence_rating  \\\n",
              "1     Mid-latitude storm tracks have likely shifted ...            medium   \n",
              "12    Human-induced climate change has contributed t...            medium   \n",
              "14    Increases in West African monsoon precipitatio...            medium   \n",
              "15    Event attribution studies and physical underst...              high   \n",
              "18    The observed average rate of heating of the cl...              high   \n",
              "...                                                 ...               ...   \n",
              "7439  Transport-related emissions in developing regi...              high   \n",
              "7442  Increased efficiency has been insufficient to ...              high   \n",
              "7493  The extent to which countries increase the amb...              high   \n",
              "7507  A significant push for international climate f...              high   \n",
              "7859  In short, future urban expansion will amplify ...         very high   \n",
              "\n",
              "     prediction  correct  \n",
              "1          high    False  \n",
              "12         high    False  \n",
              "14         high    False  \n",
              "15         high     True  \n",
              "18    very high    False  \n",
              "...         ...      ...  \n",
              "7439       high     True  \n",
              "7442       high     True  \n",
              "7493       high     True  \n",
              "7507     medium    False  \n",
              "7859       high    False  \n",
              "\n",
              "[298 rows x 4 columns]>"
            ]
          },
          "metadata": {},
          "execution_count": 147
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Precision, recall, and F1 score"
      ],
      "metadata": {
        "id": "1N1C-Gy2kcce"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, classification_report\n",
        "\n",
        "true_labels = samples['confidence_rating']\n",
        "predicted_labels = samples['prediction']\n",
        "\n",
        "# Compute macro F1 score\n",
        "f1 = f1_score(true_labels, predicted_labels, average='macro')\n",
        "print(\"Macro F1 score:\", f1)\n",
        "\n",
        "# Compute weighted F1 score\n",
        "f1 = f1_score(true_labels, predicted_labels, average='weighted')\n",
        "print(\"Weighted F1 score:\", f1)\n",
        "\n",
        "# Compute precision for each class\n",
        "precision = precision_score(true_labels, predicted_labels, average=None)\n",
        "\n",
        "# Compute recall for each class\n",
        "recall = recall_score(true_labels, predicted_labels, average=None)\n",
        "\n",
        "# Compute F1 score for each class\n",
        "f1 = f1_score(true_labels, predicted_labels, average=None)\n",
        "\n",
        "# Create a dataframe to store precision and recall for each class\n",
        "class_metrics_df = pd.DataFrame({'Precision': precision, 'Recall': recall, 'F1': f1})\n",
        "\n",
        "# Add labels to the class metrics dataframe\n",
        "class_metrics_df['Class'] = true_labels.unique().astype(str)\n",
        "\n",
        "# Sort the dataframe by class index or name\n",
        "class_metrics_df = class_metrics_df.sort_values('Class', key=lambda x: pd.Categorical(x, categories=[\"low\", \"medium\", \"high\", \"very high\"]))\n",
        "\n",
        "# Print class metrics dataframe\n",
        "print(class_metrics_df)\n",
        "\n",
        "# Compute accuracy for the whole system\n",
        "accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "\n",
        "# Compute accuracy by class\n",
        "\n",
        "print(\"Accuracy (total):\", accuracy)\n",
        "\n",
        "report = classification_report(true_labels, predicted_labels, digits=4)\n",
        "print(report)\n",
        "\n",
        "# Count classes\n",
        "category_counts = true_labels.value_counts()\n",
        "print(category_counts)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-j5UOZ21kekr",
        "outputId": "6f567cc6-bfd9-4544-c77c-39a5311b693d"
      },
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Macro F1 score: 0.29361853203091287\n",
            "Weighted F1 score: 0.3186479370663524\n",
            "   Precision    Recall        F1      Class\n",
            "2   0.169231  0.215686  0.189655        low\n",
            "0   0.384615  0.765306  0.511945     medium\n",
            "1   0.687500  0.154930  0.252874       high\n",
            "3   0.500000  0.141026  0.220000  very high\n",
            "Accuracy (total): 0.3624161073825503\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        high     0.3846    0.7653    0.5119        98\n",
            "         low     0.6875    0.1549    0.2529        71\n",
            "      medium     0.1692    0.2157    0.1897        51\n",
            "   very high     0.5000    0.1410    0.2200        78\n",
            "\n",
            "    accuracy                         0.3624       298\n",
            "   macro avg     0.4353    0.3192    0.2936       298\n",
            "weighted avg     0.4501    0.3624    0.3186       298\n",
            "\n",
            "high         98\n",
            "very high    78\n",
            "low          71\n",
            "medium       51\n",
            "Name: confidence_rating, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Over/under confidence assessment"
      ],
      "metadata": {
        "id": "ZZN3I0fxoRTc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "value_map = {'low': 0, 'medium': 1, 'high': 2, 'very high': 3}\n",
        "\n",
        "samples['predict_score'] = samples.prediction.apply(\n",
        "    lambda x: value_map[x])\n",
        "\n",
        "samples['true_score'] = samples.confidence_rating.apply(\n",
        "    lambda x: value_map[x])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1B3CwhEk87v",
        "outputId": "daf6fea8-5ddd-4e73-d131-0f2a7a2a5d63"
      },
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-153-9fc71b32cc7b>:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  samples['predict_score'] = samples.prediction.apply(\n",
            "<ipython-input-153-9fc71b32cc7b>:6: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  samples['true_score'] = samples.confidence_rating.apply(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "samples['predict_score'].mean()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r5fACIlrqRMn",
        "outputId": "ad3eb09d-e7ad-4b74-9b36-5e794d557ec5"
      },
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.7483221476510067"
            ]
          },
          "metadata": {},
          "execution_count": 154
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "samples['true_score'].mean()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jUIApH8lqbM5",
        "outputId": "163b7a82-76e6-4e1b-bc91-a9c47a2f6b5e"
      },
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.6140939597315436"
            ]
          },
          "metadata": {},
          "execution_count": 155
        }
      ]
    }
  ]
}